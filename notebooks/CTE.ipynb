{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yT3BB_3lZJuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6d1fc3-6949-483b-d9a8-f311f4593f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 105])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class CustomTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.3):\n",
        "        super(CustomTransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = F.relu\n",
        "\n",
        "    def forward(self, src):\n",
        "        src2, _ = self.self_attn(src, src, src)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "class CustomEncoderLayer(nn.Module):\n",
        "    def __init__(self, input_channel, d_model=256, nhead=4, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.vgg = VGGnet(input_channel)\n",
        "\n",
        "        # Collapse layer to flatten the output\n",
        "        self.collapse = nn.Linear(512, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layers = self._transformer_encoder_layers(d_model, nhead, num_layers)\n",
        "        self.transformer_encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "    def _transformer_encoder_layers(self, d_model, nhead, num_layers):\n",
        "        encoder_layers = []\n",
        "        for _ in range(num_layers):\n",
        "            encoder_layer = CustomTransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "            encoder_layers.append(encoder_layer)\n",
        "        return encoder_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # VGGNet feature extraction\n",
        "        x, xfeat = self.vgg(x)\n",
        "\n",
        "        # Collapse layer to flatten\n",
        "        x = self.collapse(x)\n",
        "        x = x.unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        B, L, C = x.size()\n",
        "        x = x.permute(1, 0, 2)  # (B, L, C) -> (L, B, C) for Transformer\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Transformer encoder\n",
        "        for layer in self.transformer_encoder:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # (L, B, C) -> (B, L, C) after Transformer\n",
        "\n",
        "        return x, xfeat\n",
        "\n",
        "class VGGnet(nn.Module):\n",
        "    def __init__(self, input_channel):\n",
        "        super().__init__()\n",
        "        layers = [64, 128, 256, 512]\n",
        "        self.conv1 = self._conv(input_channel, layers[0])\n",
        "        self.maxp1 = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv2 = self._conv(layers[0], layers[1])\n",
        "        self.maxp2 = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv3 = self._conv(layers[1], layers[2])\n",
        "        self.maxp3 = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv4 = self._conv(layers[2], layers[3])\n",
        "        self.maxp4 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def _conv(self, in_channels, out_channels, nlayers=2):\n",
        "        conv = []\n",
        "        for _ in range(nlayers):\n",
        "            conv.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "            conv.append(nn.BatchNorm2d(out_channels))\n",
        "            conv.append(nn.ReLU(inplace=True))\n",
        "            in_channels = out_channels\n",
        "        return nn.Sequential(*conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxp1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxp2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxp3(x)\n",
        "        xfeat = x\n",
        "        x = self.conv4(x)\n",
        "        x = self.maxp4(x)\n",
        "        x = torch.flatten(self.avg(x), 1)\n",
        "        return x, xfeat\n",
        "\n",
        "class GrnnNet(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes=105, d_model=256, nhead=4, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.encoder_layer = CustomEncoderLayer(input_channel, d_model, nhead, num_layers)\n",
        "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
        "        self.ada = nn.Linear(d_model, d_model)\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        glf, feat = self.encoder_layer(x)\n",
        "        glf = glf.squeeze(1)  # Remove sequence dimension\n",
        "\n",
        "        glfa_vertical = torch.zeros_like(glf)  # Initialize tensor for vertical mode\n",
        "        glfa_horizontal = torch.zeros_like(glf)  # Initialize tensor for horizontal mode\n",
        "\n",
        "        # Horizontal mode\n",
        "        seq_h = feat.size()[-2]\n",
        "        for n in range(seq_h):\n",
        "            patch = feat[:, :, n, :].unsqueeze(2)\n",
        "            lx = torch.flatten(self.avg(patch), 1)\n",
        "            lx = self.ada(lx)\n",
        "            glfa_horizontal += glf + lx\n",
        "\n",
        "        # Vertical mode\n",
        "        seq_v = feat.size()[-1] // 2\n",
        "        for n in range(seq_v):\n",
        "            s = 2 * n\n",
        "            patch = feat[:, :, :, s:s + 2]\n",
        "            lx = torch.flatten(self.avg(patch), 1)\n",
        "            lx = self.ada(lx)\n",
        "            glfa_vertical += glf + lx\n",
        "\n",
        "        # Combine results from both modes\n",
        "        glfa = glfa_horizontal + glfa_vertical\n",
        "\n",
        "        logits = self.classifier(glfa)\n",
        "\n",
        "        return logits\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x = torch.rand(1, 1, 64, 128)\n",
        "    mod = GrnnNet(1, 105)\n",
        "    logits = mod(x)\n",
        "    print(logits.shape)\n"
      ]
    }
  ]
}