# -*- coding: utf-8 -*-
"""CTE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HQrq1r8QtqOnhpo1XQKS9TafhEo-C83N
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# Add positional information to the input embeddings
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        # Create positional encoding matrix with sine and cosine functions
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # Add positional encoding to the input tensor
        return x + self.pe[:x.size(0), :]

# Define a custom transformer encoder layer with attention and feedforward layers.
class CustomTransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.3):
        super(CustomTransformerEncoderLayer, self).__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        # Layer normalization and dropout
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.activation = F.relu

    def forward(self, src):
        # Apply self-attention with residual connection and normalization
        src2, _ = self.self_attn(src, src, src)
        src = self.norm1(src + self.dropout1(src2))

        # Apply feedforward network with residual connection and normalization
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = self.norm2(src + self.dropout2(src2))

        return src

# Custom encoder with VGGNet, Collapse layer and transformer encoder
class CustomEncoderLayer(nn.Module):
    def __init__(self, input_channel, d_model=256, nhead=4, num_layers=4):
        super().__init__()
        self.vgg = VGGnet(input_channel)
        self.collapse = nn.Linear(512, d_model)
        self.pos_encoder = PositionalEncoding(d_model)

        # Stack multiple transformer encoder layers
        self.transformer_encoder = nn.Sequential(
            *[CustomTransformerEncoderLayer(d_model=d_model, nhead=nhead) for _ in range(num_layers)]
        )

    def forward(self, x):
        # Extract features using VGGNet and apply transformer encoding
        x, xfeat = self.vgg(x)
        x = self.collapse(x).unsqueeze(1).permute(1, 0, 2)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x).permute(1, 0, 2)
        return x, xfeat

# VGGNet for feature extraction
class VGGnet(nn.Module):
    def __init__(self, input_channel):
        super().__init__()
        layers = [64, 128, 256, 512]
        self.conv1 = self._conv(input_channel, layers[0])
        self.maxp1 = nn.MaxPool2d(2, stride=2)
        self.conv2 = self._conv(layers[0], layers[1])
        self.maxp2 = nn.MaxPool2d(2, stride=2)
        self.conv3 = self._conv(layers[1], layers[2])
        self.maxp3 = nn.MaxPool2d(2, stride=2)
        self.conv4 = self._conv(layers[2], layers[3])
        self.maxp4 = nn.MaxPool2d(2, stride=2)
        self.avg = nn.AdaptiveAvgPool2d(1)

    def _conv(self, in_channels, out_channels, nlayers=2):
        conv = []
        for _ in range(nlayers):
            conv.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))
            conv.append(nn.BatchNorm2d(out_channels))
            conv.append(nn.ReLU(inplace=True))
            in_channels = out_channels
        return nn.Sequential(*conv)

    def forward(self, x):
        # Pass input through the VGGNet layers
        x = self.conv1(x)
        x = self.maxp1(x)
        x = self.conv2(x)
        x = self.maxp2(x)
        x = self.conv3(x)
        x = self.maxp3(x)
        xfeat = x  # Save intermediate features for horizontal/vertical segmentation
        x = self.conv4(x)
        x = self.maxp4(x)
        x = torch.flatten(self.avg(x), 1)
        return x, xfeat

# Main model that combines the encoder and adds a classifier.
class GrnnNet(nn.Module):
    def __init__(self, input_channel, num_classes=105, d_model=256, nhead=4, num_layers=4):
        super().__init__()
        self.encoder_layer = CustomEncoderLayer(input_channel, d_model, nhead, num_layers)
        self.avg = nn.AdaptiveAvgPool2d(1)
        self.ada = nn.Linear(d_model, d_model)
        self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, x):
        glf, feat = self.encoder_layer(x)
        glf = glf.squeeze(1)

        # Initialize tensors for mode segmentation
        glfa_vertical = torch.zeros_like(glf)
        glfa_horizontal = torch.zeros_like(glf)

        # Horizontal segmentation
        seq_h = feat.size()[-2]
        for n in range(seq_h):
            patch = feat[:, :, n, :].unsqueeze(2)
            lx = torch.flatten(self.avg(patch), 1)
            lx = self.ada(lx)
            glfa_horizontal += glf + lx

        # Vertical segmentation
        seq_v = feat.size()[-1] // 2
        for n in range(seq_v):
            s = 2 * n
            patch = feat[:, :, :, s:s + 2]
            lx = torch.flatten(self.avg(patch), 1)
            lx = self.ada(lx)
            glfa_vertical += glf + lx

        # Concatenate and classify
        glfa = glfa_horizontal + glfa_vertical
        logits = self.classifier(glfa)
        return logits


if __name__ == '__main__':
    x = torch.rand(1, 1, 64, 128)
    mod = GrnnNet(1, 105)
    logits = mod(x)
    print(logits.shape)